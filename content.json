[{"title":"【java】泛型理解：协变、逆变、不变","date":"2021-04-03T12:55:23.000Z","path":"java/java-generic/","text":"泛型中的继承与扩展，extends 和 super 在开发中，关于泛型的操作离不开这两种实现的数据类型。使用泛型一方面是为了使程序能兼容多种数据类型，如使用继承型类 ClassA&lt;T extent B&gt; ，那么这个类可以接收 B 以及 B 的子类，而CalssA 内部能够直接对 B 进行操作，使之隔离参数B的影响；另一方面可以限定数据类型，如使用扩展型List&lt;T super C&gt; 限制输入的数据类型为C，或C的父类。 1.继承型（extends）先说结论，继承类型数据结构的最终目的是 取数据。 举例 1234567abstract class Food&#123;&#125;class Meat extends Food&#123;&#125;class Beef extends Meat&#123;&#125;class Fruit extends Food&#123;&#125;class Apple extends Fruit&#123;&#125; 当一个泛型声明为继承型，如ArrayList&lt;T extent Food&gt; list = new ArrayList&lt;?&gt;(); 那么后面声明的类型 ? 必须是Food以及Food的子类，从一定程度来说，继承型也是限制了数据的可选范围，表明接收类型的上界。 进一步说明，通过extends方式申明限制的是这个大类型，即 list这个对象，这样我们能准确地知道，从这个list中取出的具体数据类型。如 12345678910111213141516171819//正常来说，这个一般是一个赋值操作，而不是 new//一般是这样//class Adapter&lt;T extends Food&gt;&#123;// private List&lt;T&gt; list;// public Adapter(List&lt;T&gt; list)&#123;// list = this.list; // &#125;//// public T get(int index)&#123;// return (T) list.get(index);// &#125;//// public Food get(int index)&#123;// return list.get(index);// &#125;//&#125;ArrayList&lt;T extent Food&gt; list = new ArrayList&lt;Apple&gt;();Apple apple = list.get(0); 我们确切知道取出的数据类型就是 Apple。 我们并不在意数据源是如何构造，如何填充，我们只需关注的点在Adapter 中，只要你传入的数据是 Food 或者是Food 的子类，那么我们就能取出其中数据作为 Food 使用，也可以强转为其他确切子类进行操作。 2.扩展型（super）先说结论，扩展型数据结构的目的是写入数据。 对于超类来说，使用的类至少是当前声明的类，也就是声明类及它的父类。所以，声明类是所定义泛型的最低标准，即下界，而上界是它的众多父类。如 1234List&lt;? super Fruit&gt; list1 = new ArrayList&lt;Fruit&gt;();List&lt;? super Fruit&gt; list2 = new ArrayList&lt;Food&gt;();List&lt;? super Food&gt; list3 = new ArrayList&lt;Food&gt;();//只有 Food，没有其他父类 即 &lt;? super Fruit&gt; 它能接受 Fruit 本身，以及父类 Food 。 为什么说 super 的目的是 写入数据？ 从上面的写法可以看出来，使用 super 定义的 list 我们可以往里面放 Fruit 以及它的子类，它放宽了数据的限制。如 list3 也能放入 Meat 123list3.add(new Apple());list3.add(new Fruit());list3.add(new Meat()); 但是它对数据的取出操作不友好。 list1 定义的数据类型是 Fruit,list2 定义的数据类型是 Food，但是他们声明却都是 &lt;? super Fruit&gt;，那仅仅通过定义的头声明，我们无法确切知道数组存储的数据类型是什么，它可以是 Fruit，也可以是 Fruit 的任意一个子类，所以我们取出的元素推断出来的最小类型是 Fruit ，当然这也是需要进一步强转才能知道的，所以在不经强转的前提下，我们最终所知最合适的数据类型只能定义为是 Object，这也进一步推论 super 的主要目的是写入数据，而不是读取数据。 虽然我们自己编程时候知道准确的数据类型，使用时强转即可，但是在一些扩展型代码中，往往使用一个标识符指代该类型，而该标识符在编译时被擦除（编译泛型擦除），所以无法追溯，这也是 super 多使用在写数据而不是取数据的原因。 3.确切类型相对于泛型，还存在一种确切的数据类型，即不变。也就是声明具体的数据类型，写入读取都只能操作声明的类型。如 12//只能接收 String，取出的也是 String，当然也能当作 Object。万物基于 Object。Array&lt;String&gt; data = new ArrayList&lt;&gt;(); 4.协变、逆变、不变这三种其实是上面的官方描述，表达java中的继承关系。 不变上面已经解释，不多说。 协变对应 extends, 理解为使用的数据类型可以为定义的类型以及它的演化类，即子类，一个正向的推理，由父及子 逆变对应 super，使用的数据类型可以为定义的类型以及它的祖先类，即父类，逆向推理，由子及父的过程 5.总结，记忆在《Effective Java》中，有一个精炼的回答： Producer-extends, Consumer-Super, PECS。字面理解是，extends 生产者限制数据来源，super 消费者限制数据流出。 如何理解 extends 生产者限制数据来源？数据来源已被限制，所以流出端数据是安全，你只管取出使用。 super 消费者限制数据流出？ 数据流出被限制，那么数据流入端只要符合规则就可写入，也就是你只管写入。 怎么记忆 PECS 更好？ 如果以生产者来记忆，容易产生 extends 等同于生产者的错觉，正确应该把它们理解为一个组合，即 Producer（生产者）-extends（消费者），extends 的角色是消费者，它是来取数据，来消费数据的；同样类推，Consumer（消费者）-Super（生产者），super 的角色是生产者，是生产数据，主要写入数据。 以后再编写泛型的代码时，只要先确定生产关系，那么就能准确选择出是 extends 还是 super。","tags":[{"name":"java","slug":"java","permalink":"https://maxiaozhou1234.github.io/tags/java/"}]},{"title":"【note】计算机组成原理总结","date":"2021-03-19T09:11:00.000Z","path":"default/note-of-computer/","text":"计算机组成原理学习《深入浅出计算机组成原理》课程后总结 1. 冯·诺依曼体系计算机基于“冯·诺依曼体系”进行构建，主要由5部分组成，分别为： 控制器 逻辑单元（ALU） 存储器 输入 输出 2. 中央处理器 CPU从 冯·诺依曼体系 出发，控制器和算术逻辑单元组成初步的中央处理器。控制器控制着指令的读取、加载以及计数器自增，而算术逻辑单元负责指令的执行。 2.1 时钟周期CPU 主频由晶振控制，如 i5-4590 主频为3.30GHz，则表示晶振 1s 可以震荡 3.30x10^6 次，频率越高，CPU 1秒内执行的次数越多，性能越好。我们通过下面的公式来衡量 CPU 的性能： 程序的 CPU 执行时间 = 指令数×CPI×Clock Cycle Time 从公式上看，简单的提供 CPU 性能方式是提高主频，同时在 CPU 上放更多的晶体管，让 CPU 变得更快，能够处理更多的事务。但 CPU 主频不能无限提高，在 CPU 有限的空间里，为了让 CPU 更快，放置开关控制电路越多，晶体管越多，提升主频，让晶体管的“开”“关”更快，由此带来更大的耗电和发热。 CPU 功率可以用下面的公式来表示： 功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量 在开关频率和晶体管增加的情况下，为了降低功耗，最优解法是减低电压，由于电压是平方级别降低，所减小的功耗相当可观，这也是笔记本电脑通常使用低压 CPU 的原因。不过电压也不能无限减低，因为过低电压可能导致晶体管无法正常工作，从而导致 CPU 无法使用。 提高晶体管数量另一个方式，使用更高级的制芯工艺，如 5nm、7nm，这样可以在同样空间里放入更多的晶体管。 2.2 指令周期、CPU 周期（机器周期）那么，一条指令的完整执行，称之为指令周期，包括fetch-decode-excute-memory-writeback，CPU 内部不断循环执行前三个步骤，在 CPU 之外，指令执行完毕之后，需要访问内存，将数据写回主内存，由这五部分组成一个完整的 指令周期。一条指令的执行，需要先从内存中读取，而 CPU 从内存中读取一条指令的最短时间，我们称之为CPU 周期，一个完整的指令周期，至少包括指令的读取和指令的执行，即一个指令周期至少需要两个 CPU 周期，复杂的指令则需要更多的 CPU 周期。 2.3 程序的性能 程序性能 = 指令数 × CPI × 时钟周期 单指令周期处理器我们希望一整条指令的执行在一个时钟周期内完成，这样我们一个时钟周期可以执行一条指令，CPI 也为1，看起来比执行一条指令需要多个时钟周期好，采取这种思路的处理器，称为“单指令周期处理器”。由于时钟周期固定，但指令的电路复杂程度不同，所以实际上一条指令执行时间是不同的。为了保证所有指令都在一个时钟周期内完成，那么最合适的时钟周期为执行时间最长的指令，其他短执行时间的指令在执行完毕之后也需要空转等待时钟周期结束再执行下一条指令。 为了一步提升指令处理效率，引入了“指令流水线”。 2.4 指令流水线为了解决单指令周期过长问题，引入流水线，将指令周期分割为多个流水阶段（stage），实现各个阶段可以运行在不同的时钟周期中。 将一条指令拆分为多个步骤，同时每一阶段的电路在完成对应的任务之后，可以直接执行下一条指令的对应阶段。 注意：流水线设计不宜过深。理论上流水线更长，可以提高 CPU 频率，让 CPU 处理流水阶段更短，从而提高 CPU 效率。但是部分指令执行时间很短，而过长流水线使得即使是短指令也需要浪费时间在空阶段（NOP）流水寄存器上，导致实际效率降低 。同时，提高 CPU 频率，使电路更加复杂，晶体管数量增多，带来了更高的发热与功耗。 2.4.1 流水线概念及设计如果把一个指令拆分为“取指令-指令译码-执行指令”，那么它是三级的流水线；如果进一步拆分，把“执行指令”分为“指令执行-内存访问-数据写回”，那么它就是一个五级的流水线。 五级的流水线，表示在同一个时钟周期里面，可以同时运行五条指令的不同阶段。虽然对于一条指令执行时间变成了5个时钟周期，但是我们可以把CPU 主频提得更高，是单位时钟周期更小。我们不需要确保最复杂的那条指令在时钟周期里面执行完成，而只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成就好了。 如果一个操作步骤的时间太长，那么我们把这个步骤拆分为更多的步骤，让所有的步骤需要执行的时间尽量都差不多。现代 ARM 或者 Intel 的 CPU 流水线计数都已经到了 14 级。 2.4.2 超长流水线瓶颈虽然流水线可以增加吞吐率，但是流水线级数过深反而会增加性能成本。 性能开销 流水线同步时钟周期，不再是指令级别，而是流水线阶段级别。每一级流水线输出都要放入流水线寄存器中，在下一时钟周期交给下一级流水线处理，每级流水线寄存器处理大约需要 20 皮秒。过深的流水线，即使后面无需操作的阶段，也需要把时间消耗在流水线寄存器上，导致开销增大。参考奔腾4深流水线，高频率的失败。 功耗增加 流水线深度提升，必须提高主频，才能保持与之前 CPU 同样的性能。同时流水线深度增加，需要更多的晶体管，两者都让 CPU 功耗增加，使得耗电和散热都成为大问题。 2.4.3 CPU 流水线设计 结构冒险 仍使用冯·诺依曼体系结构 参考“哈佛架构”，增加指令缓存、数据缓存，解决资源冲突问题（从“硬件层面”解决问题） 插入 NOP (停顿/冒泡） 数据冒险 依赖关系 数据依赖 –先写后读 反依赖 –先读后写 输出依赖 –先写再写 NOP 操作和指令对齐 不需操作的 stage 运行一次 NOP，一定不和前一条指令的相同 stage 在同一时钟周期，这样保证前后两指令不在同一时钟周期重叠产生竞争，产生结构冒险。 NOP 对齐，有可能插入过多的 NOP 导致 CPU 空转，所以有了下面的操作数前推： 操作数前推（Operand Forwarding) 或者叫“操作数旁路”（Operand Bypassing） 前一指令与后指令有依赖关系，前指令执行结果直接传入下一指令中运行，跳过后面的写入内存操作。该操作更像是将结果转发到下一指令。 乱序执行 控制冒险 硬件加速 将条件判断、地址跳转提前到译码阶段，而不是在指令执行阶段 分支预测 静态分支预测 假装分支不发生 动态分支预测 根据之前条件跳转比较结果预测 一级分支预测/1 比特饱和计数 双模态预测器/2 比特饱和计数 2.4.4 提升 CPU 吞吐率 SuperScalar 超量发射 VLIW 超长指令字 2.4.5 CPU 指令集 CISC (Complex Instruction Set Comptuing) 复杂指令集 RISC（Reduced Instruction Set Comptuing） 精简指令集 RISC-V 开源指令集 2.5 图像处理芯片 GPU假设显示器分辨率为 640*480，那么有 30 万个像素，每秒 60 帧渲染，那么每秒需要 1800 万次单像素渲染。从栅格化开始，有3个流水线步骤，即使每个步骤只有一个指令，也需要 5400 万条指令。 所以依靠 CPU 是无法完成这个工作。因为图形渲染有固定流程，所以直接用硬件来处理这部分内容，由于计算流程固定，所以也不需要流水线停顿、乱序等导致 CPU 计算变复杂问题，使得硬件制造相对简单便宜。 图形渲染流程：图形流水线 顶点处理 – 图元处理 – 栅格处理 – 片段处理 –像素操作 顶点处理 -&gt;&gt; 三维转二维 图元处理 -&gt;&gt; 点连成图形形状 栅格处理 -&gt;&gt; 将图形形状转换为小格 片段处理 -&gt;&gt; 对栅格进行处理，色彩处理 像素操作 -&gt;&gt; 将图像片段渲染至屏 2.6 特殊用途的处理器 FPGA（Field-Programmable Gate Array）现场可编程门阵列 可通过代码编辑，使之完成特定的电路功能。芯片开发 ASIC（Application-Specific Integrated Circuit） 专用集成电路 为专门用的场景设计的芯片。如摄像头芯片、录音笔芯片等，单独处理一种场景。 特别的 ASIC – TPU 深度训练处理芯片 3. 存储器3.1 存储设备类型 SRAM(static random-access memory) 静态随机存取存储器，掉电数据小时，使用在 CPU 内存 DRAM(dynamic random acces memory) 动态随机存取存储器。密度高，容量大，相对 SRAM 便宜，需要定时刷新充电，保持数据。电脑运行内存(内存条) SSD（solid-static drive）HDD(hard disk drive) 硬盘，外部存储介质，容量更大，更便宜 3.2 数据访问优化 局部性原理 时间局部性 如果一个数据被访问了，那么它在短时间内还会被再次访问。 空间局部性 如果一个数据被访问了，那么和它相邻的数据也很快会被访问。 缓存算法 LRU（Least Recently Used） 根据需要选择合适程序架构，以及合适的存储介质 广告推荐系统（DMP）举例，详细查阅 52章《MongoDB并不是什么灵丹妙药.pdf》 数据管道：大量数据，顺序读写，响应时间不要求 Kafka,HDD 数据仓库：读多场景，超大数据，长时间存储 Arvo/Hive，HDD KV数据库：随机读写，响应高 AeriSpike ，SSD 4. 输入设备4.1 总线 双独立总线 本地总线（后端总线） 连接 CPU 和内部高速缓存 前端总线 连接 CPU 和主内存 另一种设计：三种线路和多总线架构 在 CPU 和主内存中间接入北桥芯片，一分为二 系统总线：CPU 和北桥芯片连接 内存总线：主内存和北桥芯片连接 I/O总线：北桥芯片和I/O总线连接 事实上，真实的计算机总线分更细，根据不同设备还会分成独立的 PCI 总线、ISA 总线等 4.2 信号和地址：发挥总线的价值 I/O 设备挂载在总线，两者使用映射的内存地址：内存映射IO（Memory-Mapped I/O）简称MMIO 还有通过端口通信，端口映射（Port-Mapped I/O）简称 PMIO，或者叫独立输入输出（Isolated I/O） 5. 输出设备显示器","tags":[{"name":"计算机","slug":"计算机","permalink":"https://maxiaozhou1234.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"}]},{"title":"【java】Pipe","date":"2021-03-14T10:32:17.000Z","path":"java/java-pipe/","text":"java Pipe 管道流 1. Pipe 管道对于数据流的处理，一般情况下我们在同一个线程中进行。如果遇到异步处理场景，一边进行数据写入，另一线程进行数据读取，那么 Pipe 管道可以很好解决这个问题。 先看 Pipe 的原理图示： 在同一管道中，Sink 作为头进行数据写入，而 Source 端进行数据读取，实现了数据的流通，就像是水管，一头流入，一头流出。因为 Pipe 具备了 sink 和 source，所以在不同线程中，可以通过同一个 pipe 实现一端数据写入，一端读取，从而实现了不同线程间的数据流通信。 2. 使用先打开一个 Pipe，为了模拟不同线程通信，先启动线程，读取本地文件，再写入管道，另一个线程读取，并将接收的数据打印出来。 12345678910111213141516171819202122232425262728293031323334353637383940414243&#x2F;&#x2F;开启一个通道Pipe pipe &#x3D; Pipe.open();&#x2F;&#x2F;线程：模拟数据写入CompletableFuture.runAsync(() -&gt; &#123; Pipe.SinkChannel sinkChannel &#x3D; pipe.sink(); try &#123; ByteBuffer buffer &#x3D; ByteBuffer.allocate(1024); FileChannel fileChannel &#x3D; new FileInputStream(&quot;.&#x2F;res&#x2F;normal.txt&quot;).getChannel(); &#x2F;&#x2F;读取文件中数据，写入管道 while (fileChannel.read(buffer) !&#x3D; -1) &#123; buffer.flip(); sinkChannel.write(buffer); buffer.clear(); try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;exit system&quot;); fileChannel.close(); sinkChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125;);ByteBuffer buffer &#x3D; ByteBuffer.allocate(1024);&#x2F;&#x2F;拿到 source 端，读取数据Pipe.SourceChannel source &#x3D; pipe.source();while (source.read(buffer) !&#x3D; -1) &#123; buffer.flip(); String data &#x3D; new String(buffer.array(), StandardCharsets.UTF_8); System.out.println(data); buffer.clear(); if (&quot;stop&quot;.equals(data)) &#123; System.out.println(&quot;load stop,exit!&quot;); break; &#125;&#125;source.close(); 在数据流处理角度上看，和通常使用的 InputStream OutputStream，并没有区别。 3. 小结Pipe 在开发中使用较少，写这篇的主要原因在于一天看到一篇关于 zero-copy 的文章，里面除了利用 NIO Channel 进行举例，还使用了 Pipe，让我误会 Pipe 能够进一步加速文件数据的复制操作，在了解之后发现那文章仅仅为了获取到一个 channel，与 Pipe 并没有大的关联。Pipe 就是一个单纯连接输入端和输出端的工具，仅此而已。","tags":[{"name":"java","slug":"java","permalink":"https://maxiaozhou1234.github.io/tags/java/"},{"name":"nio","slug":"nio","permalink":"https://maxiaozhou1234.github.io/tags/nio/"}]},{"title":"【java】Mapper 自定义注解器","date":"2020-07-12T06:21:09.000Z","path":"android/java-annotation/","text":"java 自定义注解器，提交私仓 maven 1. java 自定义注解器1.1 注解器作用通过元注解对类、变量等进行标记，在代码编译期通过解析器 AbstractProcessor 进行解析，快速实现模板代码的构建等作用 1.2 自定义注解通过 @interface 实现注解，如： 12345@Target(ElementType.TYPE)@Retention(RetentionPolicy.CLASS)public @interface MapType &#123; String name() default &quot;&quot;;&#125; 1.3 实现一个解析器 Processor 必须继承 AbstractProcessor，通过方法 public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment roundEnv) 对定义的注解类型进行解析。 需要重写 getSupportedAnnotationTypes() 声明对哪些些注解生效 在 init(ProcessingEnvironment processingEnv) 中获取工具类，如：12345678@Overridepublic synchronized void init(ProcessingEnvironment processingEnv) &#123; super.init(processingEnv); Types typeUtils &#x3D; processingEnv.getTypeUtils(); Elements elementUtils &#x3D; processingEnv.getElementUtils(); Filter filer &#x3D; processingEnv.getFiler(); Messager messager &#x3D; processingEnv.getMessager();&#125; 1.4 解析注解，构建类从自定义解析器可以获取并处理自定义注解类型，动态构建类可以通过 JavaWriter 或者 JavaPoet。JavaPoet 是对 JavaWriter 的进一步封装。如何使用参考官网介绍：JavaPoet github 链接 2. 实现注解库新建 java library，建议使用 idea 的 maven/gradle 工程 2.1 mapper-annotation 注解库新建 java library，命名为 mapper-annotation，用于存放注解。也可以把注解、解析器等放在同一个库中，但一般解析器所依赖的库只需要在编译期使用，不需要打包进主工程，所以会选择库拆分。 2.2 mapper-compiler 解析库同样新建 java library，再添加 JavaPoet 依赖，在 module 的 build.gradle 中添加 implementation group: &#39;com.squareup&#39;, name: &#39;javapoet&#39;, version: &#39;1.8.0&#39;在项目调试阶段，增加注解库依赖 implementation project(&quot;:mapper-annotation&quot;)，在调试完成后，可以将直接依赖替换为线上依赖，如 implementation &#39;com.lib:mapper-annotation:1.0.0&#39; 2.3 mapper aar 库这个库主要是存放对外访问的接口。给 android 项目使用，如果没有这类需求可以不加，或者也可以放入 annotation 库中，不是必要创建的库。新建 android library，同样需要添加对 annotation 依赖，在上线后替换为线上版本，如上。 3. 自定义 mapper 注解库需求及实现3.1 构建需求在业务项目中使用了大量的组件，在使用中需要根据用户的组件类型去匹配，呈现该组件。在原先开发中，流程如下： 123456789101112131415private void showComponent(Item item)&#123; String type &#x3D; item.getType(); if(&quot;A&quot;.equals(type)&#123; Component cc &#x3D; new A(item); cc.show(); &#125;else if(&quot;B&quot;.equals(type)&#123; Component cc &#x3D; new B(item); cc.show(); &#125;else if(&quot;C&quot;.equals(type)&#123; Component cc &#x3D; new C(item); cc.show(); &#125;else &#123; &#x2F;&#x2F;... &#125;&#125; 在少量组件的情况下，使用 if-else 结构清晰，但随着项目扩展，这段判断特别长，因此有了第一次改造，修改后如下： 1234567891011121314151617181920private static HashMap&lt;Strng,Class&gt; map &#x3D; new HashMap&lt;&gt;();static&#123; map.put(&quot;A&quot;,A.class); map.put(&quot;B&quot;,B.class); map.put(&quot;C&quot;,C.class); &#x2F;&#x2F;...&#125;private void showComponent(Item item)&#123; Class clz &#x3D; map.get(item.getType()); if(clz!&#x3D;null)&#123; try&#123; Component cc &#x3D; clz.getConstructor(Item.class).newInstance(item); cc.show(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 通过 HashMap 查找，让 if-else 瞬间去无踪，但是前提还是需要先构建 Map，为了一步到位，决定使用注解方式实现，彻底释放双手。 3.2 实现 定义注解因为可能存在组的概念，所以需要定义一个组名，再定义key用于标记该类，同时可能存在多个不同类型使用同一个类情况，提供一个组存放所有的类型，那么该注解完整如下所示：123456789101112131415161718@Target(ElementType.TYPE)@Retention(RetentionPolicy.CLASS)public @interface MapType &#123; &#x2F;** * 存放组 *&#x2F; String group() default Constant.defaultGroup; &#x2F;** * 用于标记的 key *&#x2F; String name() default &quot;&quot;; &#x2F;** * 多个 key 共用一个类，可以使用 array 标记 *&#x2F; String[] array() default &#123;&#125;;&#125; 定义注解用于类，并保持到源码，defalut 是默认值，由于提供了一组使用，所以允许 name 或者 array 其中一个为空，但在构建需要对 name 和 array 进行都为空判断，不允许都为空，而 group 提供一个默认组存放。 解析注解新建解析器 MapProcessor，继承 AbstractProcessor 注意：需要在 main 中创建目录 C:\\code\\AnnotationApp\\mapper-compiler\\src\\main\\resources\\META-INF\\services 并添加注解器声明文件javax.annotation.processing.Processor，内容为自定义的注解器完整类名，如下图。当然可以通过 google 提供的 AutoService 自动生成，依赖为compile group: &#39;com.google.auto.service&#39;, name: &#39;auto-service&#39;, version: &#39;1.0-rc7&#39;在 process 进行解析，并通过 JavaPoet 去生成上面改造后 map 的静态代码。部分代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192@Overridepublic boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment roundEnv) &#123; if (!annotations.isEmpty()) &#123; HashMap&lt;String, HashMap&lt;String, ClassName&gt;&gt; map &#x3D; new HashMap&lt;&gt;(); System.out.println(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; process start &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); for (Element annotatedElement : roundEnv.getElementsAnnotatedWith(MapType.class)) &#123; if (annotatedElement.getKind() &#x3D;&#x3D; ElementKind.CLASS) &#123;&#x2F;&#x2F;只对类的注解进行处理 MapType annotation &#x3D; annotatedElement.getAnnotation(MapType.class); String group &#x3D; annotation.group(); String name &#x3D; annotation.name(); String[] array &#x3D; annotation.array(); &#x2F;&#x2F;空判断，不允许都为空，key 为空字符容易出现覆盖，同时需注意同组同key有覆盖风险！！ if (name.length() &#x3D;&#x3D; 0 &amp;&amp; array.length &#x3D;&#x3D; 0) &#123; &#x2F;&#x2F;写出异常日志 log(Diagnostic.Kind.ERROR, &quot;type name and array are Empty.&quot;); continue; &#125; TypeElement typeElement &#x3D; (TypeElement) annotatedElement; ClassName className &#x3D; ClassName.get(typeElement);&#x2F;&#x2F;获取被注解的类 log(&quot;className &gt;&gt; &quot; + className); &#x2F;&#x2F;保存入临时 map HashMap&lt;String, ClassName&gt; items &#x3D; map.get(group); if (items &#x3D;&#x3D; null) &#123; items &#x3D; new HashMap&lt;&gt;(); map.put(group, items); &#125; if (name.length() &gt; 0) &#123; items.put(name, className); &#125; if (array.length &gt; 0) &#123; for (String s : array) &#123; items.put(s, className); &#125; &#125; &#125; &#125; &#x2F;&#x2F;构建静态class文件 try &#123; ClassName hashMapName &#x3D; ClassName.get(HashMap.class); ClassName stringName &#x3D; ClassName.get(String.class); ClassName className &#x3D; ClassName.get(Class.class); ParameterizedTypeName itemMap &#x3D; ParameterizedTypeName.get(hashMapName, stringName, className); ParameterizedTypeName hashMap &#x3D; ParameterizedTypeName.get(hashMapName, stringName, itemMap); &#x2F;&#x2F;创建 HashMap&lt;String,HashMap&lt;String,Class&gt;&gt; map; FieldSpec fieldMap &#x3D; FieldSpec.builder(hashMap, &quot;map&quot;, Modifier.PUBLIC, Modifier.STATIC) .initializer(&quot;new $T()&quot;, hashMap) .build(); StringBuilder body &#x3D; new StringBuilder(); CodeBlock.Builder codeBuilder &#x3D; CodeBlock.builder() .addStatement(&quot;$T item&quot;, itemMap); for (Map.Entry&lt;String, HashMap&lt;String, ClassName&gt;&gt; item : map.entrySet()) &#123; body.setLength(0); String group &#x3D; item.getKey(); System.out.println(&quot; &gt;&gt; group : &quot; + group + &quot; &lt;&lt; &quot;); codeBuilder.addStatement(&quot;item &#x3D; new $T()&quot;, itemMap); for (Map.Entry&lt;String, ClassName&gt; m : item.getValue().entrySet()) &#123; System.out.println(&quot;[ &quot; + m.getKey() + &quot;: &quot; + m.getValue()); body.append(&quot;item.put(\\&quot;&quot;).append(m.getKey()).append(&quot;\\&quot;,&quot;).append(m.getValue()).append(&quot;.class);\\n&quot;); &#125; codeBuilder.add(body.toString()) .add(&quot;map.put(\\&quot;&quot;).add(group).addStatement(&quot;\\&quot;, item)&quot;); &#125; &#x2F;&#x2F;构建静态class为 当前package.TypeMap$Data TypeSpec.Builder classBuilder &#x3D; TypeSpec.classBuilder(&quot;TypeMap$Data&quot;) .addModifiers(Modifier.PUBLIC, Modifier.FINAL) .addField(fieldMap) .addStaticBlock(codeBuilder.build()); JavaFile javaFile &#x3D; JavaFile.builder(this.getClass().getPackage().getName(), classBuilder.build()).build(); javaFile.writeTo(filer); &#125; catch (IOException e) &#123; if (e instanceof FilerException) &#123; &#x2F;&#x2F; &#125; else e.printStackTrace(); &#125; System.out.println(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; end &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); return true; &#125; return false;&#125; 这样，在添加这个注解库后，通过 build 能够动态生成 TypeMap$Data。在android 工程中，生成的目录为 \\build\\generated\\ap_generated_sources\\debug\\out\\com\\lib\\mapper，内容如下：3. 对外暴露接口在 mapper 中，定义静态类，用于链接上面生成的类。由于 TypeMap$Data 是编译期才生成，如果在没有build之前，直接引用 TypeMap$Data 的成员变量，会报错，有可能导致项目无法构建，所以通过反射方式获取到成员变量，相关代码如下： 1234567891011121314151617181920212223public class Mapper &#123; private final static HashMap&lt;String, HashMap&lt;String, Class&gt;&gt; collection; static &#123; HashMap&lt;String, HashMap&lt;String, Class&gt;&gt; temp &#x3D; null; try &#123; Class typeMap &#x3D; Class.forName(&quot;com.lib.mapper.TypeMap$Data&quot;); Object instance &#x3D; typeMap.newInstance(); Field map &#x3D; typeMap.getField(&quot;map&quot;); map.setAccessible(true); &#x2F;&#x2F;noinspection unchecked temp &#x3D; (HashMap&lt;String, HashMap&lt;String, Class&gt;&gt;) map.get(instance); &#125; catch (Exception e) &#123; e.printStackTrace(); temp &#x3D; new HashMap&lt;&gt;(); &#125; finally &#123; collection &#x3D; temp; &#125; &#125; &#x2F;&#x2F;省略其他&#125; 这样我们注解库编写完成。 4. 使用android 项目举例 添加依赖：implementation project(‘:mapper’) 声明注解器依赖：annotationProcessor project(‘:mapper-compiler’) 如果需要对 kotlin 类注解，先引入 kotlin 相关插件，再添加 apply plugin:&#39;kotlin-kapt&#39;，将 annotationProcessor 替换为 kapt，即 kapt project(&#39;:mapper-compiler&#39;) 新建类，添加注解： 1234567891011121314151617181920@MapType(name &#x3D; &quot;testA&quot;, group &#x3D; &quot;love&quot;)public class TestA &#123; public TestA() &#123; System.out.println(getClass().getName()); &#125;&#125;@MapType(name &#x3D; &quot;testB&quot;, array &#x3D; &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;)public class TestB &#123; public TestB() &#123; System.out.println(getClass().getName()); &#125;&#125;@MapType(name &#x3D; &quot;testC&quot;)public class TestC &#123; public TestC() &#123; System.out.println(getClass().getName()); &#125;&#125; 业务类中使用： 1234private void function()&#123; Class a &#x3D; Mapper.findItem(&quot;testA&quot;); &#x2F;&#x2F;具体逻辑，需做空判断&#125; 5. 提交私仓 maven为了方便其他项目使用，我们可以把库提交到私仓，这里已假设存在私仓 maven。 5.1 添加上传脚本在两个 java-library 库均添加 maven 插件，并增加上传脚本，代码块如下： 123456789101112131415161718apply plugin: &#39;maven&#39;uploadArchives &#123; repositories &#123; mavenDeployer &#123; repository(url: MAVEN_URL) &#123; authentication(userName: USER_NAME, password: USER_PASSWORD) &#125; pom.project &#123; groupId GROUP_ID&#x2F;&#x2F;定义的组 artifactId &#39;mapper-annotation&#39;&#x2F;&#x2F;库唯一标识 version &#39;1.0.0&#39;&#x2F;&#x2F;版本 packaging &#39;jar&#39;&#x2F;&#x2F;打包类型 description &#39;mapper-annotation&#39; &#125; &#125; &#125;&#125; 两个库配置相似，区别在 artifactId 和 description 不同，版本号每次上传都需要改变。特别注意，上传jar不要用 SNAPSHOT 标识，每次上传都需要修改版本，相同版本无法覆盖！！如果使用，即使提示上传成功，你在引用的时候除非指定到特定版本，如 implementation &#39;com.lib:mapper-annotation:1.0.1-20200711.111827-1&#39;,否则不能准确获取最新的版本！配置不需要增加什么 task sourceJar，除非你完全理解 gradle task，否则有可能出现无法理解问题 当然，mapper 库也需要提交，增加配置如下： 123456789101112131415161718192021222324252627282930apply plugin: &#39;maven&#39;task sourcesJar(type: Jar) &#123; baseName &quot;mapper&quot; &#x2F;&#x2F;分类器，用于区别其他jar包 classifier &quot;sources&quot; &#x2F;&#x2F;从main源集中的所有代码 from android.sourceSets.main.java.srcDirs&#125;artifacts &#123; archives sourcesJar&#125;uploadArchives &#123; repositories &#123; mavenDeployer &#123; repository(url: MAVEN_URL) &#123; authentication(userName: USER_NAME, password: USER_PASSWORD) &#125; pom.project &#123; groupId GROUP_ID artifactId &#39;mapper&#39; version &#39;1.0.0&#39; packaging &#39;aar&#39; description &#39;mapper&#39; &#125; &#125; &#125;&#125; 5.2 修改依赖 mapper-annotation 库 因为 mapper 和 mapper-compiler 均依赖注解 注解器 mapper-compiler 修改依赖注解为线上版本 对外库 mapper 修改依赖注解为线上版本 5.3 上传打开 idea gradle 控制面板，在 upload 中逐个上传。 在其他需要的项目中，添加 mapper 依赖，增加注解器即可。 项目地址，点我查阅","tags":[{"name":"java","slug":"java","permalink":"https://maxiaozhou1234.github.io/tags/java/"},{"name":"android","slug":"android","permalink":"https://maxiaozhou1234.github.io/tags/android/"}]},{"title":"【Android】无限循环ViewPager setCurrentItem 导致 ANR 分析","date":"2020-04-24T09:28:43.000Z","path":"android/viewpager-anr/","text":"无限循环 ViewPager setCurrentItem 导致 ANR 分析 1. 无限循环 ViewPager通过 adapter 的 getCount 返回一个足够大的数字，再初始化显示的item在中间位置，那么用户在左右滑动能够模拟出一个循环的显示界面。 2. 调用 setCurrentItem 卡顿当我们设置的 getCount 是一个较小的数字时，调用该方法总能快速跳转到目标位置，但是 getCount 是一个大数，如 Integer.MAX_VALUE，那么在调用跳转时，很容易触发 anr。 2.1 源码分析设置显示的item角标，最终调用 void setCurrentItemInternal(int item, boolean smoothScroll, boolean always, int velocity) 这个函数，看下这个方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344void setCurrentItemInternal(int item, boolean smoothScroll, boolean always, int velocity) &#123; if (mAdapter &#x3D;&#x3D; null || mAdapter.getCount() &lt;&#x3D; 0) &#123; setScrollingCacheEnabled(false); return; &#125; if (!always &amp;&amp; mCurItem &#x3D;&#x3D; item &amp;&amp; mItems.size() !&#x3D; 0) &#123; setScrollingCacheEnabled(false); return; &#125; if (item &lt; 0) &#123; item &#x3D; 0; &#125; else if (item &gt;&#x3D; mAdapter.getCount()) &#123; item &#x3D; mAdapter.getCount() - 1; &#125; final int pageLimit &#x3D; mOffscreenPageLimit; &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;这个设置是最后铺满画面的判定，但如果是大数，这里就是一个隐藏的 ANR &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; if (item &gt; (mCurItem + pageLimit) || item &lt; (mCurItem - pageLimit)) &#123; &#x2F;&#x2F; We are doing a jump by more than one page. To avoid &#x2F;&#x2F; glitches, we want to keep all current pages in the view &#x2F;&#x2F; until the scroll ends. for (int i &#x3D; 0; i &lt; mItems.size(); i++) &#123; mItems.get(i).scrolling &#x3D; true; &#125; &#125; final boolean dispatchSelected &#x3D; mCurItem !&#x3D; item; if (mFirstLayout) &#123; &#x2F;&#x2F; We don&#39;t have any idea how big we are yet and shouldn&#39;t have any pages either. &#x2F;&#x2F; Just set things up and let the pending layout handle things. mCurItem &#x3D; item; if (dispatchSelected) &#123; dispatchOnPageSelected(item); &#125; requestLayout(); &#125; else &#123; &#x2F;&#x2F;重要方法，添加移除item！ANR的分析重点 populate(item); &#x2F;&#x2F;滑动到目标点 scrollToItem(item, smoothScroll, velocity, dispatchSelected); &#125;&#125; 上面可以看到，首先会先根据当前位置和目标位置距离判断是否需要滑动item，如果是滑动一页，不会触发设置scrolling，假如超过 pageLimit 泽一定会设置 scrolling = true。 第二个方法是 populate(item)，用于移除看不见的item，添加新的item，下面部分代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101void populate(int newCurrentItem) &#123; &#x2F;&#x2F;...省略很多代码 &#x2F;&#x2F; Fill 3x the available width or up to the number of offscreen &#x2F;&#x2F; pages requested to either side, whichever is larger. &#x2F;&#x2F; If we have no current item we have no work to do. if (curItem !&#x3D; null) &#123; float extraWidthLeft &#x3D; 0.f; int itemIndex &#x3D; curIndex - 1; ItemInfo ii &#x3D; itemIndex &gt;&#x3D; 0 ? mItems.get(itemIndex) : null; final int clientWidth &#x3D; getClientWidth(); final float leftWidthNeeded &#x3D; clientWidth &lt;&#x3D; 0 ? 0 : 2.f - curItem.widthFactor + (float) getPaddingLeft() &#x2F; (float) clientWidth; &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;curIndex 是 mItem 的最后一个item的位置，经过上面处理，已经在原来基础上增加了一个 &#x2F;&#x2F;ii 是我们现在页面显示的item，这里处理当前页面之前的item是否需要销毁 &#x2F;&#x2F;这里也很明显看出运算次数为 pos 次，当设置是一个大数是，2^32 ≈ 8^10 约等于 10^10 &#x2F;&#x2F;那么这里将执行总数的一半次数，估计 10^9 次 &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; for (int pos &#x3D; mCurItem - 1; pos &gt;&#x3D; 0; pos--) &#123; if (extraWidthLeft &gt;&#x3D; leftWidthNeeded &amp;&amp; pos &lt; startPos) &#123; &#x2F;&#x2F;当 ii 为null时，能够跳出循环，ii的更新在下面的判断块中 if (ii &#x3D;&#x3D; null) &#123; break; &#125; &#x2F;&#x2F;只有条件成立才会更新ii，但上面说到，如果更新的位置在pageLimit之内， &#x2F;&#x2F;scrolling 为false，超出则是true，超出的时候为了保证界面能完全填充 &#x2F;&#x2F;也就是说无法跳出循环，所以在大数的时候，这里才是导致 ANR 的根本原因 if (pos &#x3D;&#x3D; ii.position &amp;&amp; !ii.scrolling) &#123; mItems.remove(itemIndex); mAdapter.destroyItem(this, pos, ii.object); if (DEBUG) &#123; Log.i(TAG, &quot;populate() - destroyItem() with pos: &quot; + pos + &quot; view: &quot; + ((View) ii.object)); &#125; itemIndex--; curIndex--; ii &#x3D; itemIndex &gt;&#x3D; 0 ? mItems.get(itemIndex) : null; &#125; &#125; else if (ii !&#x3D; null &amp;&amp; pos &#x3D;&#x3D; ii.position) &#123; extraWidthLeft +&#x3D; ii.widthFactor; itemIndex--; ii &#x3D; itemIndex &gt;&#x3D; 0 ? mItems.get(itemIndex) : null; &#125; else &#123; ii &#x3D; addNewItem(pos, itemIndex + 1); extraWidthLeft +&#x3D; ii.widthFactor; curIndex++; ii &#x3D; itemIndex &gt;&#x3D; 0 ? mItems.get(itemIndex) : null; &#125; &#125; float extraWidthRight &#x3D; curItem.widthFactor; itemIndex &#x3D; curIndex + 1; if (extraWidthRight &lt; 2.f) &#123; ii &#x3D; itemIndex &lt; mItems.size() ? mItems.get(itemIndex) : null; final float rightWidthNeeded &#x3D; clientWidth &lt;&#x3D; 0 ? 0 : (float) getPaddingRight() &#x2F; (float) clientWidth + 2.f; &#x2F;&#x2F;这里同理，上面是判断设置的新页面在右边时，对当前页的左边进行处理 &#x2F;&#x2F;下面代码是对当前页面右边的处理，一般执行次数为 pageLimit &#x2F;&#x2F;如果新页面位置在当前页的右边，下面只会执行 pageLimit 次就跳出循环，因为pos+pageLimit 后的ii是null &#x2F;&#x2F;同理，如果新页面在当前页的左边，上面也只会执行1次就跳出，因为 pos-pageLimit 的ii是null for (int pos &#x3D; mCurItem + 1; pos &lt; N; pos++) &#123; if (extraWidthRight &gt;&#x3D; rightWidthNeeded &amp;&amp; pos &gt; endPos) &#123; if (ii &#x3D;&#x3D; null) &#123; break; &#125; if (pos &#x3D;&#x3D; ii.position &amp;&amp; !ii.scrolling) &#123; mItems.remove(itemIndex); mAdapter.destroyItem(this, pos, ii.object); if (DEBUG) &#123; Log.i(TAG, &quot;populate() - destroyItem() with pos: &quot; + pos + &quot; view: &quot; + ((View) ii.object)); &#125; ii &#x3D; itemIndex &lt; mItems.size() ? mItems.get(itemIndex) : null; &#125; &#125; else if (ii !&#x3D; null &amp;&amp; pos &#x3D;&#x3D; ii.position) &#123; extraWidthRight +&#x3D; ii.widthFactor; itemIndex++; ii &#x3D; itemIndex &lt; mItems.size() ? mItems.get(itemIndex) : null; &#125; else &#123; ii &#x3D; addNewItem(pos, itemIndex); itemIndex++; extraWidthRight +&#x3D; ii.widthFactor; ii &#x3D; itemIndex &lt; mItems.size() ? mItems.get(itemIndex) : null; &#125; &#125; &#125; calculatePageOffsets(curItem, curIndex, oldCurInfo); mAdapter.setPrimaryItem(this, mCurItem, curItem.object); &#125; &#x2F;&#x2F;...省略很多代码&#125; 上面注释说明非常清楚，通过对 pos和scrolling判断，来决定是否销毁当前页的前/后数据，这里程序只会循环 currentItem 次，原本我猜测即使空转，那应该也会很快处理完成，但在大数面前，任何的空转都应当理性对待。 假设 1w 次循环耗时为0.04ms，那么被放大10^5，也会达到4s，当设置为大数时，这个循环的时间不可忽视。 为了更加严谨，我在void populate(int newCurrentItem) 前后加入时间埋点，代码如下： 123456789101112131415161718192021@Aspectpublic class AspectVp &#123; private long time &#x3D; 0; @Before(&quot;call(void populate(..))&quot;) public void beforePopulate(JoinPoint joinPoint) &#123; if (joinPoint.getArgs().length &gt; 0) &#123; Log.d(&quot;zhou&quot;, &quot;AspectVp [before populate &gt;&gt; &quot; + (time &#x3D; System.currentTimeMillis())); &#125; &#125; @After(&quot;call(void populate(..))&quot;) public void afterPopulate(JoinPoint joinPoint) throws Throwable &#123; if (joinPoint.getArgs().length &gt; 0) &#123; long t &#x3D; System.currentTimeMillis(); Log.i(&quot;zhou&quot;, &quot;AspectVp [after populate &gt;&gt; &quot; + t + &quot; &#x3D;&#x3D;&#x3D; spend &#x3D; &quot; + (t - time) + &quot; ms&quot;); &#125; &#125;&#125; 当设置个数为 20000，当前页为10000，跳转到 cur+10 位置，单次执行耗时 1127 ms，如图1所示；当设置个数为 2^32，当前页为 2^31 ，跳转到 cur+10 位置，单次执行耗时 32674193 ms，如图2所示： 图1：个数 20000 图2：个数 2^32 而且，从日志也可以看出，一次超缓存数的跳转，会触发四次 populate(item) 的调用。 1.setCurrentItem -&gt; 触发 populate(item)2.populate(item) 有新item加入 -&gt; 触发 onMeasure -&gt; populate()3.populate() -&gt; populate(item) | — 有可能再次判定加入新 item，跳转2，但下一次肯定不会有新的item需要添加 | — 没有新增的 item4.最后滑动结束，发出一个 populate() 保证页面覆盖完全 所以，3再会触发一次 populate()，但3-2-3不会成为死循环，总共有4次调用 2.2 解决思路处理这个问题，有简单的方法，因为设置大数 2^32 真的太大了，修改为小一点、用户感知不强的数字，如10000,而5千次的滑动对用户也算是大操作，并且这个循环耗时在一个可接受范围，也不会造成页面的卡顿甚至 ANR。 或者，当设置的item超过pageLimit，我们强制把 isSrolling 设置为false，那么在遍历缓存 mItem 时能够及时更新 ii，使我们及时打破循环，跳出无用的循环时间。 3. 具体方案：打破循环打破循环，让 for (int pos = mCurItem - 1; pos &gt;= 0; pos--) 和 for (int pos = mCurItem + 1; pos &lt; N; pos++) 尽快结束循环 3.1 设置有限的小数（相对 2^32 来说）adapter 设置 getCount 为小数值，让循环基数降低，即使执行次数多，所等待的时间也处于可接受范围 1234567891011class Adapter extends PagerAdapter &#123; &#x2F;&#x2F;...省略其他代码 @Override public int getCount() &#123; return 10000;&#x2F;&#x2F;自行设置一个合理的数值 &#125; &#x2F;&#x2F;...省略其他代码&#125; 3.2 不触发设置 scrolling 条件在 setCurrentItem 后，只要设置的 newIndex 在区间 (currentItem-pageLimit,currentItem+pageLimit)，就不会触发设置该条件，那么在调用设置之前，把 pageLimit 设置为 Math.abs(newIndex - currentItem)，调用设置位置之后，再重置回去，同样可以达到秒跳转效果，代码如下： 12345678910111213&#x2F;&#x2F;原调用 viewPager.setCurrentItem(newIndex, true); 修改如下int tmp &#x3D; viewPager.getOffscreenPageLimit();int newIndex &#x3D; viewPager.getCurrentItem() + 10; int newLimit &#x3D; Math.abs(newIndex-viewPager.getCurrentItem());if(newLimit&gt;tmp) &#123; viewPager.setOffscreenPageLimit(newLimit); viewPager.setCurrentItem(newIndex, true); viewPager.setOffscreenPageLimit(tmp);&#125;else&#123; viewPager.setCurrentItem(newIndex, true);&#125; 3.3 重置 scrolling 为 false在设置完 setCurrentItem 后，由于跳转距离问题会将 scrolling 置为 true，所以在执行 void populate(int newCurrentItem) 之前把 scrolling 重置为 false，但是 mItems 是私有变量，需通过反射获取，再通过 AspectJ 埋点在执行之前遍历重置 scrolling，这么看来，无疑方法二是最快解决问题的方式。 以下代码仅供参考，请不要随意应用于生产环境，确定使用请仔细评估性能消耗，完成覆盖测试 ！注意：这里的注入对象是所有的 ViewPager！！ 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Aspectpublic class AspectVp &#123; private long time &#x3D; 0; private static ArrayList&lt;Object&gt; items &#x3D; null; &#x2F;&#x2F;反射获取 public static void setupViewPager(ViewPager viewPager) &#123; try &#123; Field field &#x3D; viewPager.getClass().getDeclaredField(&quot;mItems&quot;); field.setAccessible(true); items &#x3D; (ArrayList&lt;Object&gt;) field.get(viewPager); &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123; e.printStackTrace(); &#125; Log.i(&quot;zhou&quot;, &quot;setupViewPager &quot; + (items &#x3D;&#x3D; null ? &quot;null&quot; : items.size())); &#125; &#x2F;&#x2F;及时销毁 public static void destroyItems() &#123; items &#x3D; null; Log.i(&quot;zhou&quot;, &quot;destroyItems &quot;); &#125; @Before(&quot;call(void populate(..))&quot;) public void beforePopulate(JoinPoint joinPoint) &#123; if (joinPoint.getArgs().length &gt; 0) &#123; Log.d(&quot;zhou&quot;, &quot;AspectVp [before populate &gt;&gt; &quot; + (time &#x3D; System.currentTimeMillis())); &#125; &#125; @After(&quot;call(void populate(..))&quot;) public void afterPopulate(JoinPoint joinPoint) throws Throwable &#123; if (joinPoint.getArgs().length &gt; 0) &#123; long t &#x3D; System.currentTimeMillis(); Log.i(&quot;zhou&quot;, &quot;AspectVp [after populate &gt;&gt; &quot; + t + &quot; &#x3D;&#x3D;&#x3D; spend &#x3D; &quot; + (t - time) + &quot; ms&quot;); &#125; &#125; @Before(&quot;execution(void populate(..))&quot;) public void executionBefore(JoinPoint joinPoint) throws Throwable &#123; if (joinPoint.getArgs().length &gt; 0 &amp;&amp; items !&#x3D; null) &#123; for (Object obj : items) &#123;&#x2F;&#x2F;强制重置为false Field scrolling &#x3D; obj.getClass().getDeclaredField(&quot;scrolling&quot;); scrolling.setAccessible(true); scrolling.set(obj, false); &#125; Log.i(&quot;zhou&quot;, &quot;executionBefore [&quot; + items.size() + &quot;]&quot;); &#125; &#125;&#125;&#x2F;&#x2F;调用class MainActivity extent Activity&#123; @Override protected void onCreate(@Nullable Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); &#x2F;&#x2F;省略其他 AspectVp.setupViewPager(viewPager); &#125; @Override protected void onStop() &#123; super.onStop(); &#x2F;&#x2F;销毁 AspectVp.destroyItems(); &#125;&#125; 运行效果如下： 完。","tags":[{"name":"android","slug":"android","permalink":"https://maxiaozhou1234.github.io/tags/android/"}]},{"title":"【Android】PagerAdapter不刷新问题分析","date":"2020-04-22T14:50:10.000Z","path":"android/pager-adapter/","text":"使用 PagerAdapter notifyDataSetChange 不刷新问题分析无论是普通的 ViewPager 视图，还是用 Fragment，当我们刷新数据后调用 notifyDataSetChange 后，往往会发现当前界面并没有预想的触发刷新，根本原因在于 ViewPager 的缓存机制判定数据未发生变化，从而不触发刷新，及时数据确实发生了改变。 1. 未触发刷新效果及分析假设当前 page 是第一页，第一个数据发生变更，此时调用 notifyDataSetChange 后页面并没有变化，原因在于当前页面没有触发强制刷新，仅仅是从缓存中取数据而已。 1.1 不刷新模拟效果ViewPager 默认缓存数是1，即当前页+缓存，总共2，从效果图也可以看出更新数据后，当前页和滑动一页并不会销毁改页，在两页之后回到第一页，之前的页才被销毁，重新创建。 此页面有三个 view 在 ViewPager 中，当点击按钮会更新第一个 view 中的文字，在没有重写 getItemPosition 下，效果如下所示 1.2 不刷新源码分析当然，我们可以从 ViewPager 中看到，当我们调用 notifyDataSetChange 后会回调 VP 的 void dataSetChanged()，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142void dataSetChanged() &#123; &#x2F;&#x2F; This method only gets called if our observer is attached, so mAdapter is non-null. final int adapterCount &#x3D; mAdapter.getCount(); mExpectedAdapterCount &#x3D; adapterCount; boolean needPopulate &#x3D; mItems.size() &lt; mOffscreenPageLimit * 2 + 1 &amp;&amp; mItems.size() &lt; adapterCount; int newCurrItem &#x3D; mCurItem; boolean isUpdating &#x3D; false; for (int i &#x3D; 0; i &lt; mItems.size(); i++) &#123; final ItemInfo ii &#x3D; mItems.get(i); &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;核心方法&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; final int newPos &#x3D; mAdapter.getItemPosition(ii.object); if (newPos &#x3D;&#x3D; PagerAdapter.POSITION_UNCHANGED) &#123; continue; &#125; if (newPos &#x3D;&#x3D; PagerAdapter.POSITION_NONE) &#123; mItems.remove(i); i--; if (!isUpdating) &#123; mAdapter.startUpdate(this); isUpdating &#x3D; true; &#125; mAdapter.destroyItem(this, ii.position, ii.object); needPopulate &#x3D; true; if (mCurItem &#x3D;&#x3D; ii.position) &#123; &#x2F;&#x2F; Keep the current item in the valid range newCurrItem &#x3D; Math.max(0, Math.min(mCurItem, adapterCount - 1)); needPopulate &#x3D; true; &#125; continue; &#125; &#x2F;&#x2F;...省略一大堆 &#125; &#x2F;&#x2F;...省略一大堆&#125; 首先会遍历所有缓存，通过 mAdapter.getItemPosition(obj) 判断是否需要销毁重建，该方法默认值 POSITION_UNCHANGED 为-1，即默认数据没有变化。所以我们只需要重写这个方法，确定当前位置的数据是否应该销毁，当然有很多 demo 都是直接建议销毁，即返回 POSITION_NONE。本身 ViewPager 设计并不是为频繁变化的数据，所以数据变化频繁或者为了性能更好，尽可能使用 RecyclerView + PagerSnapHelper 替换 VP。 1.3 重写 adapter 中方法实现刷新好，为了继续使用 VP 来更新，应该动态的判断这个 getItemPosition，本身 VP 性能一般，一刀切不适合这种场景，所以我们通过 setTag 来解决这个数据关联问题，如下示例： 1234567891011121314151617181920212223242526272829private List&lt;HashMap&lt;String, String&gt;&gt; data &#x3D; new ArrayList&lt;&gt;();class Adapter extends PagerAdapter &#123; &#x2F;&#x2F;...省略其他方法 @NonNull @Override public Object instantiateItem(@NonNull ViewGroup container, int position) &#123; TextView view &#x3D; (TextView) LayoutInflater.from(ViewPagerActivity.this).inflate(R.layout.layout_simple_text, container, false); String text &#x3D; data.get(position).get(&quot;name&quot;); view.setText(text); view.setTag(-1, position); view.setTag(-2, text); container.addView(view); return view; &#125; @Override public int getItemPosition(@NonNull Object object) &#123; if (getCount() &#x3D;&#x3D; 0) return POSITION_UNCHANGED; View view &#x3D; (View) object; int pos &#x3D; (int) view.getTag(-1); String msg &#x3D; (String) view.getTag(-2); if (pos &gt;&#x3D; getCount()) &#123; return POSITION_NONE; &#125; return msg.equals(data.get(pos).get(&quot;name&quot;)) ? POSITION_UNCHANGED : POSITION_NONE; &#125;&#125; 更改之后效果图： 在判断之后，数据修改之后可以及时反馈到界面上，代价是需要数据源标记原始数据位置，但仅仅是一个4位的int 2. 为什么还要使用 PagerAdapter？ViewPager 在普通的 View 页面，在今天，使用空间其实很小了，但在 Fragment 组合页面，配合 FragmentPagerAdapter，不得不说非常方便，如果在存在需要动态增减 Fragment，使用 getItemPosition（obj） 来减少创建销毁还是比较合适，当然使用场景是固定几个 fragment （2-3个）配合setOffscreenPageLimit缓存可以一次创建足够，如果有一堆页面（超过设置的缓存数量），这个创建销毁过程的消耗还是客观存在，不容小觑。 3. FragmentPagerAdapter 改造，更适合 Fragment 增减3.1 Fragment 创建使用 FragmentPagerAdapter 有页面替换需要，除了重写 getItemPosition() 还需要重写 getItemId(position)。为什么？因为 fragment 存在于 FragmentManager 中，通过 mFragmentManager.findFragmentByTag(name) 来找到之前的 fragment，也可以理解为** fragment 的缓存**，具体源码代码如下： 123456789101112131415161718192021222324252627@Overridepublic Object instantiateItem(ViewGroup container, int position) &#123; if (mCurTransaction == null) &#123; mCurTransaction = mFragmentManager.beginTransaction(); &#125; final long itemId = getItemId(position); // Do we already have this fragment? String name = makeFragmentName(container.getId(), itemId); Fragment fragment = mFragmentManager.findFragmentByTag(name); if (fragment != null) &#123; if (DEBUG) Log.v(TAG, \"Attaching item #\" + itemId + \": f=\" + fragment); mCurTransaction.attach(fragment); &#125; else &#123; fragment = getItem(position); if (DEBUG) Log.v(TAG, \"Adding item #\" + itemId + \": f=\" + fragment); mCurTransaction.add(container.getId(), fragment, makeFragmentName(container.getId(), itemId)); &#125; if (fragment != mCurrentPrimaryItem) &#123; fragment.setMenuVisibility(false); fragment.setUserVisibleHint(false); &#125; return fragment;&#125; 如果没有重写该方法，那么，在增减数据后，即使 getItemPosition 判定数据变化，再通过 makeFragmentName(container.getId(), itemId) （itemId 默认是position）获取的 tag 还是不变，那么重新拿到的 fragment 和原来位置的 fragment 一样，，所以必须通过重写 getItemId 来修改 tag，使这数据源中的 fragment 和这个 tag 形成唯一个关联关系，一般唯一性用hashCode就足够了。示例代码如下： 1234567891011121314151617181920212223242526272829303132333435363738private List&lt;Fragment&gt; data = new ArrayList&lt;&gt;();class PageAdapter extends FragmentPagerAdapter &#123; PageAdapter(FragmentManager fm) &#123; super(fm); &#125; @Override public Fragment getItem(int position) &#123; SimpleFragment fragment = (SimpleFragment) data.get(position); fragment.setPosition(position); return fragment; &#125; @Override public int getCount() &#123; return data.size(); &#125; @Override public long getItemId(int position) &#123; return data.get(position).hashCode(); &#125; @Override public int getItemPosition(@NonNull Object object) &#123; if (getCount() == 0) return POSITION_UNCHANGED; int position = ((SimpleFragment) object).getPosition(); if (position &gt;= getCount()) &#123; return POSITION_NONE; &#125; return data.get(position).hashCode() == object.hashCode() ? POSITION_UNCHANGED : POSITION_NONE; &#125;&#125; 注： SimpleFragment 只是继承 Fragment 增加一个 position 参数及相应方法。 3.2 适配器修改前后效果图示效果图：操作中的删除为 data.remove(1);//删除第二个数据 1.仅修改 getItemPosition 2.修改 getItemPosition 和 getItemId 完。","tags":[{"name":"android","slug":"android","permalink":"https://maxiaozhou1234.github.io/tags/android/"}]},{"title":"【sqlite】测试题","date":"2020-02-26T06:54:21.000Z","path":"sqlite/sqlite-test/","text":"数据库测试题 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 数据库测试，写出对应的操作 sql 语句1.筛选出表A和表B中列【id,name,age】去重后合并输出2.表1和表2存在id关联关系，将两表列合并输出所有数据（无重复数据）3.表1和表2彼此独立，请列出两表的所有可能组合4.表1为员工表，manager_id 为该员工领导id，请通过单表列出员工的[id,name,manager_name]======= 数据库测试，写出对应的操作 sql 语句1.筛选出表A和表B中列【id,name,age】去重后合并输出合并选择 union/union all 去重 union前提两个表均包含列【id,name,age】 select id,name,age from table_A union select id,name,age from table_B; 2.表1和表2存在id关联关系，将两表列相关联数据全部输出 存在关联，明显是用 join 对于单表都有扩展的列，可以用 outer join，inner join 只要相关联数据，说明是两表都有的数据，使用 inner join。使用 outer join 输出会把缺少的补位 null，属于全量。 123456789101112131415mysql&gt; select * from company;+----+-------+------+------------+--------+| id | name | age | address | salary |+----+-------+------+------------+--------+| 1 | Paul | 32 | California | 20000 || 2 | Allen | 25 | Texas | 15000 || 3 | Teddy | 23 | Norway | 20000 || 4 | Mark | 25 | Rich-Mond | 65000 || 5 | David | 27 | Texas | 85000 || 6 | Kim | 22 | South-Hall | 45000 || 7 | James | 24 | Houston | 10000 || 8 | Kit | 30 | NY | 12000 || 10 | Mike | 17 | NY | 2000 |+----+-------+------+------------+--------+9 rows in set (0.00 sec) 12345678910111213mysql&gt; select * from department;+----+-------------+--------+| id | dept | emp_id |+----+-------------+--------+| 1 | IT Billing | 1 || 2 | Engineering | 2 || 3 | Finance | 7 || 4 | Engineering | 3 || 5 | Finance | 4 || 6 | Engineering | 5 || 7 | Finance | 6 |+----+-------------+--------+7 rows in set (0.00 sec) 使用 inner join 输出： mysql&gt; select c.id,c.name,c.salary,d.dept from company c inner join department d on c.id=d.emp_id; 123456789101112+----+-------+--------+-------------+| id | name | salary | dept |+----+-------+--------+-------------+| 1 | Paul | 20000 | IT Billing || 2 | Allen | 15000 | Engineering || 7 | James | 10000 | Finance || 3 | Teddy | 20000 | Engineering || 4 | Mark | 65000 | Finance || 5 | David | 85000 | Engineering || 6 | Kim | 45000 | Finance |+----+-------+--------+-------------+7 rows in set (0.00 sec) 对比外连接1.使用 company 左外连接 department mysql&gt; select c.id,c.name,c.salary,d.dept from company c left outer join department d on c.id=d.emp_id; 1234567891011121314+----+-------+--------+-------------+| id | name | salary | dept |+----+-------+--------+-------------+| 1 | Paul | 20000 | IT Billing || 2 | Allen | 15000 | Engineering || 3 | Teddy | 20000 | Engineering || 4 | Mark | 65000 | Finance || 5 | David | 85000 | Engineering || 6 | Kim | 45000 | Finance || 7 | James | 10000 | Finance || 8 | Kit | 12000 | NULL || 10 | Mike | 2000 | NULL |+----+-------+--------+-------------+9 rows in set (0.00 sec) 2.使用 company 右外连接 department （sqlite不支持右外连接可以使用左外连接，调整两个表位置） mysql&gt; select c.id,c.name,c.salary,d.dept from company c right outer join department d on c.id=d.emp_id; 123456789101112+------+-------+--------+-------------+| id | name | salary | dept |+------+-------+--------+-------------+| 1 | Paul | 20000 | IT Billing || 2 | Allen | 15000 | Engineering || 7 | James | 10000 | Finance || 3 | Teddy | 20000 | Engineering || 4 | Mark | 65000 | Finance || 5 | David | 85000 | Engineering || 6 | Kim | 45000 | Finance |+------+-------+--------+-------------+7 rows in set (0.00 sec) 上面可以看到，外连接输出与左表、右表相关，题目只要相关联，使用inner join。 3.表1和表2彼此独立，请列出两表的所有可能组合交叉连接select * from table_1 cross join table_2; 4.表1为员工表，manager_id 为该员工领导id，请通过单表列出员工的[id,name,manager_name]mysql&gt; desc employ; 123456789+------------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+------------+----------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | char(20) | YES | | NULL | || salary | double | YES | | NULL | || manager_id | int(11) | YES | | NULL | |+------------+----------+------+-----+---------+-------+4 rows in set (0.01 sec) mysql&gt; select * from employ; 1234567891011+------+------+--------+------------+| id | name | salary | manager_id |+------+------+--------+------------+| 1 | aa | 2000 | 3 || 2 | bb | 1000 | 3 || 3 | cc | 2500 | 4 || 4 | dd | 6000 | 100 || 100 | ee | 6000 | 0 || 5 | vv | 3000 | 100 |+------+------+--------+------------+6 rows in set (0.00 sec) mysql&gt; select e.id,e.name,e2.name as manager_name from employ e,employ e2 where e.manager_id=e2.id; 12345678910+------+------+--------------+| id | name | manager_name |+------+------+--------------+| 1 | aa | cc || 2 | bb | cc || 3 | cc | dd || 4 | dd | ee || 5 | vv | ee |+------+------+--------------+5 rows in set (0.00 sec) 5.触发器，表1是用户数据表，uid是操作者id（假设是运维管理），id是新增用户id，需要把表1的增删改操作记录到表log中[uid操作者,action动作,id被操作对象,date时间]表1：create table user (id int primary key auto_increment,name char(20),age int check(age&gt;0),uid int); mysql&gt; desc user; 123456789+-------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | || age | int(11) | YES | | NULL | || uid | int(11) | YES | | NULL | |+-------+----------+------+-----+---------+----------------+4 rows in set (0.01 sec) 表2：create table user_log (uid int,action char(20),id int,time timestamp default CURRENT_TIMESTAMP); mysql&gt; desc user_log; 123456789+--------+-----------+------+-----+-------------------+-------+| Field | Type | Null | Key | Default | Extra |+--------+-----------+------+-----+-------------------+-------+| uid | int(11) | YES | | NULL | || action | char(20) | YES | | NULL | || id | int(11) | YES | | NULL | || time | timestamp | NO | | CURRENT_TIMESTAMP | |+--------+-----------+------+-----+-------------------+-------+4 rows in set (0.01 sec) 创建触发器。由于 MySQL 5.5只支持单事件触发，所以需要三个对应触发器 增 mysql&gt; create trigger user_insert after insert on user -&gt; for each row -&gt; BEGIN -&gt; insert into user_log(uid,action,id) values (new.uid,’create’,new.id); -&gt; END; -&gt; //Query OK, 0 rows affected (0.06 sec) 改 mysql&gt; create trigger user_update after update on user -&gt; for each row -&gt; BEGIN -&gt; insert into user_log(uid,action,id) values (old.uid,’update’,old.id); -&gt; END; -&gt; //Query OK, 0 rows affected (0.05 sec) 删 mysql&gt; create trigger user_delete after delete on user -&gt; for each row -&gt; BEGIN -&gt; insert into user_log(uid,action,id) values (old.uid,’delete’,old.id); -&gt; END; -&gt; //Query OK, 0 rows affected (0.08 sec) 测试： mysql&gt; select * from user; Empty set (0.00 sec) mysql&gt; select * from user_log; Empty set (0.00 sec) mysql&gt; insert into user (uid,name,age)values (1000,&apos;aka&apos;,10); Query OK, 1 row affected (0.03 sec) mysql&gt; insert into user (uid,name,age)values (1000,&apos;mille&apos;,19); Query OK, 1 row affected (0.03 sec) mysql&gt; insert into user (uid,name,age)values (1003,&apos;jim&apos;,22); Query OK, 1 row affected (0.03 sec) mysql&gt; update user set name=&apos;Sare&apos; where id=&apos;1&apos;; Query OK, 1 row affected (0.02 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; delete from user where id=2; Query OK, 1 row affected (0.03 sec)结果显示：mysql&gt; select * from user; 1234567+----+-------+------+------+| id | name | age | uid |+----+-------+------+------+| 1 | Sare | 10 | 1000 || 3 | jim | 22 | 1003 |+----+-------+------+------+2 rows in set (0.00 sec) mysql&gt; select * from user_log; 12345678910+------+--------+------+---------------------+| uid | action | id | time |+------+--------+------+---------------------+| 1000 | create | 1 | 2020-02-26 10:59:26 || 1000 | create | 2 | 2020-02-26 10:59:43 || 1003 | create | 3 | 2020-02-26 11:00:12 || 1000 | update | 1 | 2020-02-26 11:01:02 || 1000 | delete | 2 | 2020-02-26 11:01:39 |+------+--------+------+---------------------+5 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2","tags":[{"name":"sqlite","slug":"sqlite","permalink":"https://maxiaozhou1234.github.io/tags/sqlite/"}]},{"title":"【sqlite】VIEW 视图","date":"2020-02-25T10:08:50.000Z","path":"sqlite/sqlite-view/","text":"sqlite view 视图 View 视图视图（View）可以包含一个表的所有行或从一个或多个表选定行。视图（View）可以从一个或多个表创建，这取决于要创建视图的 SQLite 查询。 1.创建视图语法 CREATE [TEMP | TEMPORARY] VIEW view_name ASSELECT column1, column2…..FROM table_nameWHERE [condition]; 2.创建视图实例你可以只从一个表中取其中几列做为新视图，这里用两个表合并为一个视图。 mysql&gt; create view company_view as select e.id,e.name,e.age,d.dept from company e inner join department d where e.id=d.emp_id;Query OK, 0 rows affected (0.03 sec) mysql&gt; select * from company_view;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD id name age dept ======= 1234567891011121314151617+----+-------+------+-------------+| id | name | age | dept |+----+-------+------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | 32 | IT Billing || 2 | Allen | 25 | Engineering || 7 | James | 24 | Finance || 3 | Teddy | 23 | Engineering || 4 | Mark | 25 | Finance || 5 | David | 27 | Engineering || 6 | Kim | 22 | Finance |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD7 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+----+-------+------+-------------+7 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 3.删除视图 drop view view_name mysql&gt; show tables;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Tables_in_learn_db ======= 123456789101112131415+--------------------+| Tables_in_learn_db |+--------------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| company || company_view || department || employ || log |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD5 rows in set (0.01 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+--------------------+5 rows in set (0.01 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 mysql&gt; drop view company_view;Query OK, 0 rows affected (0.00 sec) mysql&gt; show tables;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Tables_in_learn_db ======= 12345678910111213+--------------------+| Tables_in_learn_db |+--------------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| company || department || employ || log |+--------------------+&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD4 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;4 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2","tags":[{"name":"sqlite","slug":"sqlite","permalink":"https://maxiaozhou1234.github.io/tags/sqlite/"}]},{"title":"【sqlite】Trigger 触发器","date":"2020-02-25T05:34:25.000Z","path":"sqlite/sqlite-trigger/","text":"sqlite Trigger 触发器 Trigger 触发器SQLite 触发器（Trigger）是数据库的回调函数，它会在指定的数据库事件发生时自动执行/调用。指定在特定的数据库表发生 DELETE、INSERT 或 UPDATE 时触发，或在一个或多个指定表的列发生更新时触发。 Trigger 详细资料链接 1.语法 CREATE TRIGGER trigger_name [BEFORE|AFTER] event_name[INSERT|UPDATE|DELETE] ON table_nameFOR EACH ROW（对 SQLite 可加可不加）BEGIN – 触发器逻辑….END; 2.SQLite 使用触发器1）创建记录表，用于存储触发后数据create table log(uid int,entry_date text); 2)创建触发器create TRIGGER emp_log AFTER INSERT ON employBEGININSERT INTO log(uid,rtime) values (new.id,datetime(‘now’,’localtime’));END; 3)向 employ 插入数据，查看log中记录 insert into employ values(5,’vv’,3000,100); 因为没有创建新表，所以存在之前的数据，旧数据再触发器之前存在是不会新增记录在 log 表中 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| id | name | salary | manager_id ||-:|:-:|:-|======= 12345678910111213141516171819202122232425+------+------+--------+------------+| id | name | salary | manager_id |+------+------+--------+------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | aa | 2000 | 3 || 2 | bb | 1000 | 3 || 3 | cc | 2500 | 4 || 4 | dd | 6000 | 100 || 100 | ee | 6000 | 0 || 5 | vv | 3000 | 100 |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD6 rows in set (0.00 sec)log 中记录&gt; select * from log;| uid | entry_date||-:|:-:||5 | 2020-02-25 13: 07:02 |1 row in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+------+------+--------+------------+6 rows in set (0.00 sec) log 中记录 select * from log; 123456+------+----------------------+| uid | entry_date |+------+----------------------+|5 | 2020-02-25 13: 07:02 |+------+----------------------+1 row in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 2.MySQL 使用触发器使用 MySQL 添加触发器语法和上面一致注意: 如果是在命令行中执行MySQL，需要将delimiter ; 更改为delimiter //，MySQL 默认使用分号 “;” 结束判断，不更改在遇到分号执行语句会导致提示语法错误导致创建触发器失败 需要增加 FOR EACH ROW， MySQL 不支持语句触发器,仅仅支持行级触发器，必须增加这句 示例如下： mysql&gt; delimiter //mysql&gt; CREATE TRIGGER emp_log AFTER INSERT ON employ -&gt; FOR EACH ROW -&gt; BEGIN -&gt; INSERT INTO log(uid) values (new.id); -&gt; END; -&gt; //Query OK, 0 rows affected (0.08 sec) 最后可以重新把分号执行修改回来，在命令行输入 mysql&gt; delimiter ; 3.MySQL 触发器查询，删除&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD#####１)查询所有触发器======= 1)查询所有触发器 44295f6b3470b9af3de24b5f8792d7898b2cbfa2show triggers; 2)删除触发器 drop trigger xxx; (xxx 为触发器名称）","tags":[{"name":"sqlite","slug":"sqlite","permalink":"https://maxiaozhou1234.github.io/tags/sqlite/"}]},{"title":"【sqlite】unions 语法笔记","date":"2020-02-25T03:30:03.000Z","path":"sqlite/sqlite-union/","text":"SQLite unions 合并查询 SQLite的 UNION 子句/运算符用于合并两个或多个 SELECT 语句的结果，不返回任何重复的行。为了使用 UNION，每个 SELECT 被选择的列数必须是相同的，相同数目的列表达式，相同的数据类型，并确保它们有相同的顺序，但它们不必具有相同的长度。 SQLite unions 合并查询将表1和表2的内容合并输出，前提是两个表中的列必须一致(列名、顺序) 1.内联查询 mysql&gt; select emp_id,name,dept from company inner join department on company.id=department.emp_id; &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| emp_id | name | dept ||-:|:-:|:-|======= 1234567891011121314151617+--------+-------+-------------+| emp_id | name | dept |+--------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 7 | James | Finance || 3 | Teddy | Engineering || 4 | Mark | Finance || 5 | David | Engineering || 6 | Kim | Finance |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD7 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+--------+-------+-------------+7 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 2.左外连接查询 mysql&gt; select emp_id,name,dept from company left outer join department on company.id=department.emp_id; &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| emp_id | name | dept ||-:|:-:|:-|======= 1234567891011121314151617+--------+-------+-------------+| emp_id | name | dept |+--------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 3 | Teddy | Engineering || 4 | Mark | Finance || 5 | David | Engineering || 6 | Kim | Finance || 7 | James | Finance |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD7 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+--------+-------+-------------+7 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 3.联合输出 mysql&gt; select emp_id,name,dept from company inner join department on company.id=department.emp_id union select emp_id,name,dept from company left outer join department on company.id=department.emp_id; &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| emp_id | name | dept ||-:|:-:|:-|======= 123456789101112131415161718192021222324+--------+-------+-------------+| emp_id | name | dept |+--------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 7 | James | Finance || 3 | Teddy | Engineering || 4 | Mark | Finance || 5 | David | Engineering || 6 | Kim | Finance |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD7 rows in set (0.00 sec)上面这个例子结果不能很好展现合并的效果，我们更改一下第二条查询的条件，增加员工id&lt;5，如下&gt; mysql&gt;select emp_id,name,dept from company left outer join department on company.id&#x3D;department.emp_id and company.id&lt;5;| emp_id | name | dept ||-:|:-:|:-|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+--------+-------+-------------+7 rows in set (0.00 sec) 上面这个例子结果不能很好展现合并的效果，我们更改一下第二条查询的条件，增加员工id&lt;5，如下 mysql&gt;select emp_id,name,dept from company left outer join department on company.id=department.emp_id and company.id&lt;5; 1234567891011121314151617+--------+-------+-------------+| emp_id | name | dept |+--------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 3 | Teddy | Engineering || 4 | Mark | Finance || NULL | David | NULL || NULL | Kim | NULL || NULL | James | NULL |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD7 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+--------+-------+-------------+7 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 这里我们看到左外连接后，id&gt;=5的员工数据被置为null，此时再联合输出： mysql&gt; select emp_id id,name,dept from company inner join department on company.id=department.emp_id union select emp_id,name,dept from company left outer join department on company.id=department.emp_id and company.id&lt;5; &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| id | name | dept ||-:|:-:|:-|======= 123456789101112131415161718192021222324+------+-------+-------------+| id | name | dept |+------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 7 | James | Finance || 3 | Teddy | Engineering || 4 | Mark | Finance || 5 | David | Engineering || 6 | Kim | Finance || NULL | David | NULL || NULL | Kim | NULL || NULL | James | NULL |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD10 rows in set (0.00 sec)查询后可以看到，表1的查询结果都在，表2中1-4数据和表1相同合并为一，剩下三条再并表输出，总共数据为7+3 &#x3D; 10条。#### 4.union all 合并所有&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+------+-------+-------------+10 rows in set (0.00 sec) 查询后可以看到，表1的查询结果都在，表2中1-4数据和表1相同合并为一，剩下三条再并表输出，总共数据为7+3 = 10条。 4.union all 合并所有 44295f6b3470b9af3de24b5f8792d7898b2cbfa2仅使用 union 关键字，重复的项会被过滤，如果要保留两个表中的所有内容，可以使用union all合并所有数据。将上面的例子修改一下： mysql&gt; select emp_id id,name,dept from company inner join department on company.id=department.emp_id union all select emp_id,name,dept from company left outer join department on company.id=department.emp_id and company.id&lt;5; &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD| id | name | dept ||-:|:-:|:-|======= 123456789101112131415161718192021222324+------+-------+-------------+| id | name | dept |+------+-------+-------------+&gt;&gt;&gt;&gt;&gt;&gt;&gt; 44295f6b3470b9af3de24b5f8792d7898b2cbfa2| 1 | Paul | IT Billing || 2 | Allen | Engineering || 7 | James | Finance || 3 | Teddy | Engineering || 4 | Mark | Finance || 5 | David | Engineering || 6 | Kim | Finance || 1 | Paul | IT Billing || 2 | Allen | Engineering || 3 | Teddy | Engineering || 4 | Mark | Finance || NULL | David | NULL || NULL | Kim | NULL || NULL | James | NULL |&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD14 rows in set (0.00 sec)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+------+-------+-------------+14 rows in set (0.00 sec) 44295f6b3470b9af3de24b5f8792d7898b2cbfa2 结果如上。","tags":[{"name":"sqlite","slug":"sqlite","permalink":"https://maxiaozhou1234.github.io/tags/sqlite/"}]},{"title":"【sqlite】join 语法笔记","date":"2020-02-25T02:01:00.000Z","path":"sqlite/sqlite-join/","text":"SQLite 数据库 join 语法笔记 SQLite 关键字 join1.cross join 交叉连接把第一个表的每一行和第二个表的每一行进行匹配。如表1有m行，表2有n行，交叉连接之后有 m*n 行。 select * from company; select * from department; select emp_id,name,dept from company cross join department; 2.inner join 内联根据连接条件结合两个表的列值创建一个新的结果表。内连接语法： SELECT … FROM table1 [INNER] JOIN table2 ON conditional_expression … 为了避免冗余，并保持较短的措辞，可以使用 USING 表达式声明内连接（INNER JOIN）条件。这个表达式指定一个或多个列的列表：注：如果两张表的关联字段名相同，才可以使用USING子句 SELECT … FROM table1 JOIN table2 USING ( column1 ,… ) … 自然连接（NATURAL JOIN）类似于 JOIN…USING，只是它会自动测试存在两个表中的每一列的值之间相等值： SELECT … FROM table1 NATURAL JOIN table2… 示例：select emp_id,name,dept from company inner join department on company.id=department.emp_id; 3.outer join 外连接外连接（OUTER JOIN）是内连接（INNER JOIN）的扩展。虽然 SQL 标准定义了三种类型的外连接：LEFT、RIGHT、FULL，但 SQLite 只支持 左外连接（LEFT OUTER JOIN）。 左连接 left outer join/left join关联两张或多张表中，根据条件显示匹配的数据。依附于左表进行扩展，扩展后表中内容如果右表没有匹配数据，则用 null 显示。 右连接 right outer join/right join关联两张或多张表中，根据条件显示匹配的数据。依附于右表进行扩展，扩展后表中内容如果左表没有匹配数据，则用 null 显示。 全连接 full outer join/right join全外连接就是关联的两张或多张表中，根据关联条件，显示所有匹配和不匹配的记录。左表中有的记录，但是右表中没有匹配上的，以空(null)显示。右表中有的记录，但是左表中没有匹配上的，也以空(null)显示。FULL OUTER JOIN也可以简写成FULL JOIN，效果是一样的。 左外连接语法： SELECT … FROM table1 LEFT OUTER JOIN table2 ON [conditional_expression 条件] … 为了避免冗余，可以使用 using 声明条件 SELECT … FROM table1 LEFT OUTER JOIN table2 USING ( column1 ,… ) … 示例：select emp_id,name,dept from company left outer join department on company.id=department.emp_id; 4.自连接，扩展只有一张表，通过把表取别名，当作两张表使用，自己和自己关联。 示例：查询经理的名称，通过员工id和经理id相同自连接查询 12345678910111213create table employ (id int,name char(20),salary real check(salary &gt;0),manager_id int);insert into employ values (1,&#39;aa&#39;,2000,3);insert into employ values (2,&#39;bb&#39;,1000,3);insert into employ values (3,&#39;cc&#39;,2500,4);insert into employ values (4,&#39;dd&#39;,6000,100);insert into employ values (100,&#39;ee&#39;,6000,0);默认使用 inner join，同样也可以使用left joinselect e.name,e.salary,e2.name manager from employ e ，employ e2 on e.manager_id &#x3D; e2.id;使用条件 whereselect e.name,e.salary,e2.name manager from employ e , employ e2 where e.manager_id &#x3D; e2.id; 图片存储记录：1.png2.png6.png4.png5.png3.png","tags":[{"name":"sqlite","slug":"sqlite","permalink":"https://maxiaozhou1234.github.io/tags/sqlite/"}]},{"title":"【MySQL】操作命令","date":"2020-02-25T02:00:00.000Z","path":"sqlite/mysql-cmd/","text":"MySQL 基本操作命令 MySQL 基本操作命令 查看所有数据库show databases; 使用数据库use xxx; 创建数据库create database xxx charset=utf8;//创建xx数据库，并指定编码 查看创建数据库时的语法命令show create database xxx; 查看数据库中所有的表show tables; 删除数据库drop database xxx;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD MySQL 5.5版本 没有 datatime,如需要使用时间戳，用timestmap代替 默认使用分号；结束判断，如果需要多行输入，将delimiter ; 更改为delimiter //，按“//”为结束判断 待办记录 SQLite PRAGMA SQLite 约束 SQLite Join SQLite Unions 子句 SQLite NULL 值 （null 未知） is not null is null SQLite 别名 as 省略后效果一致 SQLite 触发器 SQLite 索引 SQLite Indexed By 不明 SQLite Alter 命令 SQLite Truncate Table SQLite 视图 SQLite 事务 SQLite 子查询 SQLite Autoincrement SQLite 注入 SQLite Explain SQLite Vacuum SQLite 日期 &amp; 时间 SQLite 常用函数 查看所有触发器show triggers; 查看指定表触发器（使用 \\G 切换视图为纵向输出）select * from information_schema.triggers where EVENT_OBJECT_TABLE=’table_name’; 删除触发器drop trigger trigger_name; 查看表结构desc table;describe table; 创建视图create view view_name as select column1[…] from table where [condition]; 删除视图drop view name; MySQL 5.5版本注意 没有 datatime,如需要使用时间戳，用timestmap代替 默认使用分号；结束判断，如果需要多行输入，将delimiter ; 更改为delimiter //，按“//”为结束判断 没有 datetime ，时间戳使用 timestamp 44295f6b3470b9af3de24b5f8792d7898b2cbfa2","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://maxiaozhou1234.github.io/tags/MySQL/"}]},{"title":"【java】NIO 小结","date":"2020-02-11T09:15:45.000Z","path":"java/java-nio/","text":"java NIO 学习后的小结 1. NIO 总述nio 为 Non-blocking io，即不阻塞io操作，java在为并发提供的 io 操作类，主要有三个核心类，分别为： Channel 操作数据通道 Buffer 缓冲数据区域 Selector 用于管理 channel 2. BIO 与 NIO 的主要区别2.1 面向操作BIO 是面向流操作，NIO 是面向缓冲操作。BIO 每次从流读写一个或多个字节，直至所有字节被读写完成，该过程数据没有被缓存到其它地方，它不能前后移动流中的数据。 NIO 将数据先缓冲到稍后处理的区域，需要时可以在缓冲区前后移动，具备处理过程中的灵活性。 2.2 阻塞与非阻塞Java IO 流失阻塞的，意味着，当线程调用 read（）或 write（）时，该线程被阻塞，直到数据完全读取或者写入，期间线程无法进行其它处理。 NIO 的非阻塞模式，可以让线程请求写入一些数据到某通道，但不需要等到操作完成，这个现场同时可以去做其他事情。线程通常将非阻塞IO空闲时间用于其他通道上执行IO操作，所以一个线程可以管理多个输入、输出通道。 2.3 选择器NIO 的选择器允许一个单独线程监视多个输入通道，可以注册多个通道使用一个选择器，然后监控可以处理的输入通道进行操作。 3. NIO 中的 channel3.1 FileChannelFileChannel 可以通过 RandomAccessFile.getChannel() 或 InputStream,OutputStream .getChannel() 获取，示例代码如下 12345678910111213141516171819202122232425private void channelCopy() &#123; Instant begin = Instant.now(); try &#123; RandomAccessFile source = new RandomAccessFile(\"./res/threeWithoutPunctuation\", \"r\"); RandomAccessFile target = new RandomAccessFile(\"./res/copyFileNio\", \"rw\"); ByteBuffer buffer = ByteBuffer.allocate(1024*8); FileChannel sourceChannel = source.getChannel(); FileChannel targetChannel = target.getChannel(); while (sourceChannel.read(buffer) != -1) &#123; buffer.flip(); while (buffer.hasRemaining()) &#123; targetChannel.write(buffer); &#125; buffer.clear(); &#125; sourceChannel.close(); targetChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(\"[channelCopy] Done &gt;&gt; \" + (Duration.between(begin, Instant.now()).toMillis()) + \" ms\");&#125; 小结：测试在小文件复制速度可能不如流操作，但在大文件拷贝速度比流复制快，测试拷贝1.03G文件，channel 耗时 1.08s，而 stream 需要 11.31s。 3.2 DatagramChannelDatagramChannel 广播包的操作，区别不大，示例代码如下： 服务端 12345678910111213141516171819202122232425private SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.getDefault());private void startServer() &#123; try &#123; DatagramChannel datagramChannel = DatagramChannel.open(); datagramChannel.configureBlocking(true); datagramChannel.socket().bind(new InetSocketAddress(8000)); System.out.println(\"启动服务端\"); ByteBuffer buffer = ByteBuffer.allocate(1024); SocketAddress socketAddress; while (true) &#123; //超过buffer大小部分将被丢弃 if ((socketAddress = datagramChannel.receive(buffer)) != null) &#123; buffer.flip(); System.out.println(Charset.forName(\"UTF-8\").decode(buffer)); buffer.clear(); datagramChannel.send(Charset.forName(\"UTF-8\").encode(\"服务端已收到[\" + format.format(new Date())), socketAddress); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 客户端 123456789101112131415161718192021222324252627282930313233343536373839public DatagramNioClient() &#123; try &#123; DatagramChannel datagramChannel = DatagramChannel.open(); datagramChannel.socket().bind(new InetSocketAddress(8001)); new Thread(() -&gt; &#123; try &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); while (true) &#123; if (datagramChannel.receive(buffer) != null) &#123; buffer.flip(); System.out.print(\"收到消息：\"); System.out.println(Charset.forName(\"UTF-8\").decode(buffer)); buffer.clear(); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); System.out.println(\"启动客户端\"); Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) &#123; send(datagramChannel, scanner.nextLine()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125;private void send(DatagramChannel datagramChannel, String msg) &#123; try &#123; datagramChannel.send(Charset.forName(\"UTF-8\").encode(msg), new InetSocketAddress(\"127.0.0.1\", 8000)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 需注意，如果接收数据超过设定容器的大小，超过部分会丢弃。比如对方发送了128K数据，而我容器只有48K大小，那么我只接收到48K数据，而剩余部分直接被丢弃。 对于数据的读取可以传入 ByteBuffer[] 数据，将按照顺序进行填充，对于一些固定大小数据头的数据包，使用非常方便，缺点容量一旦确定不可修改，弹性差，示例： 1234567891011private void start()&#123; try&#123; DatagramChannel datagramChannel = DatagramChannel.open(); datagramChannel.connect(new InetSocketAddress(\"127.0.0.1\",8001)); ByteBuffer headBuffer = ByteBuffer.allocate(48); ByteBuffer bodyBuffer = ByteBuffer.allocate(128); datagramChannel.read(new ByteBuffer[]&#123;headBuffer,bodyBuffer&#125;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 这里的 connect 并非真正的建立连接，而是锁定 channel 通道，让该通道只能通过特定的地址收发数据。 3.3 SocketChannel,ServerSocketChannel这两个 SocketChannel 是 socket 的并发版本，通常我们通过 ServerSocket 来监听端口，一旦有客户端连接，就创建线程进行通信，示例如下： 123456789101112131415private void server() throws IOException &#123; ServerSocket serverSocket = new ServerSocket(8000); Socket socket; while ((socket = serverSocket.accept()) != null) &#123; final Socket ss = socket; new Thread(() -&gt; &#123; try &#123; InputStream in = ss.getInputStream(); OutputStream out = ss.getOutputStream(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 每一次连接都需要创建线程进行通信，所以服务端线程数与客户端数量呈1:1关系，线程的创建需要消耗服务器资源，而服务器资源有限，在并发高且传输数据小的环境，这种方式无法满足要求。 SocketChannel 能够很好的解决高并发下的资源问题，通过 Selector 注册后，在非阻塞模式下仅使用单线程可以管理多个通道并实现数据传输，服务端示例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private void server() throws IOException &#123; //建立服务端监听，设置为非阻塞模式 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(8000)); //将 channel 注册到 selector Selector selector = Selector.open(); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; //检查是否有可处理通道 int num = selector.select(); if (num == 0) continue;//jdk 中空转有一定几率造成 cpu 100% Set&lt;SelectionKey&gt; set = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = set.iterator(); while (it.hasNext()) &#123; SelectionKey key = it.next(); it.remove(); if (key.isAcceptable()) &#123;//把通道注册为可读 ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel acceptChannel = channel.accept(); acceptChannel.configureBlocking(false); acceptChannel.register(selector,SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123;//处理已连接 channel 数据 SocketChannel channel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); if(channel.read(buffer)!=-1)&#123; buffer.flip(); System.out.println(Charset.forName(\"UTF-8\").decode(buffer)); //do something channel.write(Charset.forName(\"UTF-8\").encode(\"回复\")); &#125; channel.close(); &#125; &#125; &#125;&#125; 从上面可以看出，SocketChannel 在并发量大处理的优越性。 如果是需要管理成千上万个连接，并且这些连接每次只是发送少量的数据，如聊天服务器这类需求，实现NIO服务器可能是一个优势，但如果是少量连接使用，一次发送大量数据，还是典型的IO服务器实现更符合要求。 当然 SocketChannel 还是存在缺点，比如注释中写到可能出现cpu占用100%的bug（说已修复但仍有小概率出现），api使用比较复杂，但对于小型的服务处理，NIO服务器仍然是一个高效可用的实现。","tags":[{"name":"java","slug":"java","permalink":"https://maxiaozhou1234.github.io/tags/java/"},{"name":"nio","slug":"nio","permalink":"https://maxiaozhou1234.github.io/tags/nio/"}]},{"title":"【其他】gittalk 配置问题","date":"2020-02-11T06:58:05.000Z","path":"default/gittalk/","text":"使用 gittalk 为 Hexo 添加评论功能，遇到问题及解决方法 1.申请及配置1.1 注册 gittalk可通过 Register a new OAuth application 进行注册。如果已经注册过，可以在 github 首页点击头像下拉，“Settings – Developer settings – OAuth Apps” 查看你的app，选择你注册的 app 进行再次编辑。 1.2 配置填写 Application name： 应用名称，随意 Homepage URL： 网站URL，对应自己博客地址 Application description ：描述，随意 Authorization callback URL：# 网站URL，博客地址就好 点击注册，页面会出现其中Client ID和Client Secret在后面的配置中需要用到 如我的 gittalk 填写如下： 1234Application name： CommentAppHomepage URL： https:&#x2F;&#x2F;maxiaozhou1234.github.io # 网站URL，对应自己博客地址Application description ：repo # 描述，随意Authorization callback URL：https:&#x2F;&#x2F;maxiaozhou1234.github.io # 网站URL，博客地址就好，如果有独立域名，可填写你的域名用于跳转 1.3 在主题的 _config.yml 进行配置123456789101112#Cmmentscomment: gitalk: enable: true ## 开启gitalk owner: ## GitHub的用户名 repo: ## 此评论存放的GitHub仓库 client_id: ## 复制刚才生成的clientID，例如. 75752dafe7907a897619 client_secret: ## 复制刚才生成的clientSecret，例如. ec2fb9054972c891289640354993b662f4cccc50 admin: ## Github的用户名 id: location.pathname language: zh-CN ## Language pagerDirection: last # Comment sorting direction, available values are last and first. 主题的配置，可以参考【Hexo博客折腾】BlueLake博客主题的详细配置 2. 搭建过程遇到的问题2.1 评论区显示 Error: Not Found遇到 Error: Not Found，这个问题是主题 _config.yml 中 gittalk 配置中 repo 填写错误，修改为你的博客主页即可，如我的博客配置如下： repo： maxiaozhou1234.github.io 2.2 博客评论登录跳转到首页问题申请配置是填写的Homepage URL,Authorization callback URL 不正确导致，第一个填博客首页，第二个是授权回调页面，因为我没有使用独立的域名，所以两个都填博客首页，如下 Homepage URL：https://maxiaozhou1234.github.ioAuthorization callback URL：https://maxiaozhou1234.github.io 如果你是有自己独立的域名，将 Authorization callback URL 填写为你的域名，前提是你已完成了域名的绑定，还有注意 https 和 http 区别，需完全一致。 参考文章：解决配置gitalk插件后初始化登录时跳转回首页","tags":[{"name":"gittalk","slug":"gittalk","permalink":"https://maxiaozhou1234.github.io/tags/gittalk/"},{"name":"hexo","slug":"hexo","permalink":"https://maxiaozhou1234.github.io/tags/hexo/"}]}]